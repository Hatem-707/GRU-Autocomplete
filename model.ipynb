{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abfd7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'vocab_path': 'word2id.json',\n",
    "    'id_path': 'id2word.json',\n",
    "    'emb_path': 'embedding_matrix_daily.npz',\n",
    "    'batch_size': 512,\n",
    "    'hidden_dim': 512,       # Reverted to 256 as requested\n",
    "    'num_layers': 3,         # IMPROVEMENT: Increased layers for depth\n",
    "    'learning_rate': 0.001,\n",
    "    'max_seq_len': 30, \n",
    "    'pad_token_id': 0,\n",
    "    'dropout': 0.3,\n",
    "    'unk_token_id': 0        # Placeholder, will update after loading vocab\n",
    "}\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f40bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK ID set to: 2\n",
      "Vocab Size: 20003, Embedding Dim: 300\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary\n",
    "with open(CONFIG['vocab_path'], 'r') as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "with open(CONFIG['id_path'], 'r') as f:\n",
    "    id2word = json.load(f)\n",
    "\n",
    "# Update UNK ID in config\n",
    "CONFIG['unk_token_id'] = int(word2id.get('<UNK>', 0))\n",
    "print(f\"UNK ID set to: {CONFIG['unk_token_id']}\")\n",
    "\n",
    "# Load Embedding Matrix\n",
    "emb_data = np.load(CONFIG['emb_path'])\n",
    "embedding_matrix = emb_data[list(emb_data.keys())[0]] \n",
    "\n",
    "vocab_size, embed_dim = embedding_matrix.shape\n",
    "print(f\"Vocab Size: {vocab_size}, Embedding Dim: {embed_dim}\")\n",
    "\n",
    "# Convert to Tensor (Float32 for safety, Mixed Precision trainer will handle casting)\n",
    "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4590ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPredictionDataset(Dataset):\n",
    "    def __init__(self, encoded_sentences, unk_id, max_len=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoded_sentences: List of list of token IDs\n",
    "            unk_id: ID of the <UNK> token\n",
    "            max_len: Max context window\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        for sentence in encoded_sentences:\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Create N-grams\n",
    "            for i in range(1, len(sentence)):\n",
    "                input_seq = sentence[:i]\n",
    "                target_token = sentence[i]\n",
    "                \n",
    "                # IMPROVEMENT: Skip sample if target is UNK\n",
    "                # This prevents the model from learning to predict <UNK>\n",
    "                if target_token == unk_id:\n",
    "                    continue\n",
    "                \n",
    "                if len(input_seq) > max_len:\n",
    "                    input_seq = input_seq[-max_len:]\n",
    "                \n",
    "                self.samples.append((input_seq, target_token))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target = self.samples[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=CONFIG['pad_token_id']\n",
    "    )\n",
    "    targets = torch.stack(targets)\n",
    "    return padded_inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6250f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch, hidden_dim] (The final state of the GRU)\n",
    "        # encoder_outputs: [batch, seq_len, hidden_dim] (All states of the GRU)\n",
    "        \n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden state src_len times\n",
    "        hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(hidden_expanded + encoder_outputs))\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e4)\n",
    "            \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b417b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordGRU(pl.LightningModule):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, vocab_size, lr, pad_idx):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['embedding_matrix'])\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        # FROZEN as requested because they are FastText semantics\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix, \n",
    "            freeze=True, \n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # 2. GRU Layer (Reverted to GRU)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_matrix.shape[1], \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=CONFIG['num_layers'], # 3 Layers\n",
    "            batch_first=True,\n",
    "            dropout=CONFIG['dropout'] if CONFIG['num_layers'] > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 3. Attention Layer\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # 4. Dense Output\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != self.hparams.pad_idx)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # GRU Output\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        \n",
    "        # Take the hidden state of the LAST layer\n",
    "        final_hidden = hidden[-1] \n",
    "        \n",
    "        # Calculate Attention\n",
    "        attn_weights = self.attention(final_hidden, outputs, mask)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), outputs).squeeze(1)\n",
    "        \n",
    "        # Combine Context and Hidden\n",
    "        combined = torch.cat((context, final_hidden), dim=1)\n",
    "        logits = self.fc(combined)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predictions == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7526b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^\\w]+$', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71febd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading compined.txt...\n",
      "Loaded 380481 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_and_encode_corpus(filename, word2id, unk_token='<UNK>', eos_token='<EOS>'):\n",
    "    encoded_corpus = []\n",
    "    unk_id = int(word2id.get(unk_token, 0)) \n",
    "    eos_id = int(word2id.get(eos_token, 2))\n",
    "    \n",
    "    print(f\"Reading {filename}...\")\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            if not text: continue\n",
    "            words = text.split()\n",
    "            encoded_sent = []\n",
    "            for w in words:\n",
    "                w = preprocess_word(w)\n",
    "                if not w: continue\n",
    "                val = word2id.get(w, unk_id)\n",
    "                encoded_sent.append(int(val))\n",
    "            if encoded_sent:\n",
    "                encoded_sent.append(eos_id)\n",
    "                encoded_corpus.append(encoded_sent)\n",
    "    print(f\"Loaded {len(encoded_corpus)} sentences.\")\n",
    "    return encoded_corpus\n",
    "\n",
    "# Ensure word2id values are ints\n",
    "word2id = {k: int(v) for k, v in word2id.items()}\n",
    "\n",
    "corpus_ints = load_and_encode_corpus(\n",
    "    'compined.txt', \n",
    "    word2id, \n",
    "    unk_token='<UNK>', \n",
    "    eos_token='<EOS>'\n",
    ")\n",
    "\n",
    "split_idx = int(len(corpus_ints) * 0.8)\n",
    "train_data = corpus_ints[:split_idx]\n",
    "val_data = corpus_ints[split_idx:]\n",
    "\n",
    "# --- DATASETS WITH UNK SKIPPING ---\n",
    "train_dataset = TextPredictionDataset(\n",
    "    train_data, \n",
    "    unk_id=CONFIG['unk_token_id'], \n",
    "    max_len=CONFIG['max_seq_len']\n",
    ")\n",
    "val_dataset = TextPredictionDataset(\n",
    "    val_data, \n",
    "    unk_id=CONFIG['unk_token_id'], \n",
    "    max_len=CONFIG['max_seq_len']\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss', dirpath='./checkpoints', filename='daily-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=2, mode='min'\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=3, verbose=True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# --- MODEL & TRAINING ---\n",
    "model = NextWordGRU(\n",
    "    embedding_matrix=embedding_tensor,\n",
    "    hidden_dim=CONFIG['hidden_dim'],\n",
    "    vocab_size=vocab_size,\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    pad_idx=CONFIG['pad_token_id']\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    precision='16-mixed', # ENABLE MIXED PRECISION\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7467d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /home/hatem/Development/python/ml/autocomplete/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name      | Type             | Params | Mode  | FLOPs\n",
      "---------------------------------------------------------------\n",
      "0 | embedding | Embedding        | 6.0 M  | train | 0    \n",
      "1 | gru       | GRU              | 4.4 M  | train | 0    \n",
      "2 | attention | Attention        | 263 K  | train | 0    \n",
      "3 | fc        | Linear           | 20.5 M | train | 0    \n",
      "4 | loss_fn   | CrossEntropyLoss | 0      | train | 0    \n",
      "---------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "6.0 M     Non-trainable params\n",
      "31.2 M    Total params\n",
      "124.677   Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a84f22fc624468a7ca3c596b6477f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e4a902e2f441e4bb104322c3ea2ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087765b7ece6493eaa751c55b457c2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 5.242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73986b8a7ce447b9828ea5e0a68c813a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.184 >= min_delta = 0.0. New best score: 5.058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634b3f6f890c40febab896529a7b4b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.076 >= min_delta = 0.0. New best score: 4.982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27aa458e32b54355ac7cacae4a80d3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 4.961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688ce34e360f43808c2ea6dbf66e2d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 4.960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5815cf7aa3e24deea965c9f599c56c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b7005eb66e468bba9a69ec107911ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b301aed578bd471ab7841809b998f0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 4.960. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4702e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 3104585 samples\n",
      "Val dataset: 840192 samples\n"
     ]
    }
   ],
   "source": [
    "# --- PREPARE DATA FOR EVALUATION (WITHOUT TRAINING) ---\n",
    "# Re-use existing 'train_data' and 'val_data' lists from previous cells\n",
    "# Just create datasets without running trainer\n",
    "\n",
    "split_idx = int(len(corpus_ints) * 0.8)\n",
    "train_data = corpus_ints[:split_idx]\n",
    "val_data = corpus_ints[split_idx:]\n",
    "\n",
    "train_dataset = TextPredictionDataset(\n",
    "    train_data, \n",
    "    unk_id=CONFIG['unk_token_id'], \n",
    "    max_len=CONFIG['max_seq_len']\n",
    ")\n",
    "val_dataset = TextPredictionDataset(\n",
    "    val_data, \n",
    "    unk_id=CONFIG['unk_token_id'], \n",
    "    max_len=CONFIG['max_seq_len']\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa7e9558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Fine-Tuner...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /home/hatem/Development/python/ml/autocomplete/checkpoints_ft exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode  | FLOPs\n",
      "---------------------------------------------------------------\n",
      "0 | embedding | Embedding        | 6.0 M  | train | 0    \n",
      "1 | gru       | GRU              | 4.4 M  | train | 0    \n",
      "2 | attention | Attention        | 263 K  | train | 0    \n",
      "3 | fc        | Linear           | 20.5 M | train | 0    \n",
      "4 | loss_fn   | CrossEntropyLoss | 0      | train | 0    \n",
      "---------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "6.0 M     Non-trainable params\n",
      "31.2 M    Total params\n",
      "124.677   Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-Tuning (Sequence Length 4)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4482683a11148c78bda1a2961f2dea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683e75cd4b2d427fb3e08225b989e688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5937d9d954684c87a1e2d4b147ec371b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b231cea7fd64df884915dbd95bc7397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5afef7879974f4aacd5df25b6034190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DATASET FOR MULTI-WORD PREDICTION ---\n",
    "class MultiWordDataset(Dataset):\n",
    "    def __init__(self, encoded_sentences, unk_id, pad_id, max_len=20, pred_len=4):\n",
    "        self.samples = []\n",
    "        self.pred_len = pred_len\n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        for sentence in encoded_sentences:\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Create samples\n",
    "            # We stop earlier so we don't start a sample at the very end\n",
    "            for i in range(1, len(sentence)):\n",
    "                input_seq = sentence[:i]\n",
    "                \n",
    "                # Get the next 'pred_len' tokens as target\n",
    "                target_seq = sentence[i : i + pred_len]\n",
    "                \n",
    "                # If target is UNK, we might want to skip, but for seqs it's complex.\n",
    "                # Let's just keep them but rely on the model to learn.\n",
    "                \n",
    "                # Pad target sequence if it hits EOS early\n",
    "                if len(target_seq) < pred_len:\n",
    "                    padding = [pad_id] * (pred_len - len(target_seq))\n",
    "                    target_seq = target_seq + padding\n",
    "                \n",
    "                if len(input_seq) > max_len:\n",
    "                    input_seq = input_seq[-max_len:]\n",
    "                \n",
    "                self.samples.append((input_seq, target_seq))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.samples[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "def multi_collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad inputs (variable length)\n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=CONFIG['pad_token_id']\n",
    "    )\n",
    "    # Targets are fixed length (4), so we just stack them\n",
    "    targets = torch.stack(targets) \n",
    "    return padded_inputs, targets\n",
    "\n",
    "# --- 2. PREPARE NEW DATALOADERS ---\n",
    "# Re-use existing 'train_data' and 'val_data' lists from previous cells\n",
    "ft_train_dataset = MultiWordDataset(train_data, unk_id=CONFIG['unk_token_id'], pad_id=CONFIG['pad_token_id'])\n",
    "ft_val_dataset = MultiWordDataset(val_data, unk_id=CONFIG['unk_token_id'], pad_id=CONFIG['pad_token_id'])\n",
    "\n",
    "ft_train_loader = DataLoader(ft_train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=multi_collate_fn, num_workers=0)\n",
    "ft_val_loader = DataLoader(ft_val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=multi_collate_fn, num_workers=0)\n",
    "\n",
    "\n",
    "# --- 3. FINE-TUNING MODEL WRAPPER ---\n",
    "class MultiStepFineTuner(NextWordGRU):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, targets = batch # src: [Batch, Seq], targets: [Batch, 4]\n",
    "        \n",
    "        # --- Step 1: Initial Encode (Identical to Forward) ---\n",
    "        embedded = self.embedding(src)\n",
    "        encoder_outputs, hidden = self.gru(embedded) # hidden is (Layers, Batch, Dim)\n",
    "        \n",
    "        loss = 0\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        # Current hidden state for the Attention/Prediction layer (Top layer of stack)\n",
    "        current_hidden = hidden[-1]\n",
    "        \n",
    "        # --- Step 2: Decode Loop (4 Steps) ---\n",
    "        # We use the previous hidden state to predict, then feed the TRUE target (Teacher Forcing)\n",
    "        # to generate the next hidden state.\n",
    "        \n",
    "        mask = (src != self.hparams.pad_idx)\n",
    "        \n",
    "        for i in range(targets.size(1)):\n",
    "            # A. PREDICTION\n",
    "            # Attend to the ORIGINAL Context (Seq2Seq style)\n",
    "            attn_weights = self.attention(current_hidden, encoder_outputs, mask)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            combined = torch.cat((context, current_hidden), dim=1)\n",
    "            logits = self.fc(combined)\n",
    "            \n",
    "            # B. LOSS CALCULATION\n",
    "            step_target = targets[:, i]\n",
    "            step_loss = self.loss_fn(logits, step_target)\n",
    "            loss += step_loss\n",
    "            \n",
    "            # C. PREPARE NEXT STEP (Teacher Forcing)\n",
    "            # Feed the TRUE current token into the GRU to get state for next prediction\n",
    "            # Embed the target token: [Batch] -> [Batch, 1, Emb]\n",
    "            inp_next = self.embedding(step_target).unsqueeze(1)\n",
    "            \n",
    "            # Update Hidden State\n",
    "            # We feed the FULL hidden stack (layers, batch, dim) back into GRU\n",
    "            _, hidden = self.gru(inp_next, hidden)\n",
    "            current_hidden = hidden[-1]\n",
    "            \n",
    "        # Average loss over the steps\n",
    "        final_loss = loss / targets.size(1)\n",
    "        self.log('train_loss', final_loss, prog_bar=True)\n",
    "        return final_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Simplified validation: just check loss on sequences\n",
    "        src, targets = batch\n",
    "        embedded = self.embedding(src)\n",
    "        encoder_outputs, hidden = self.gru(embedded)\n",
    "        loss = 0\n",
    "        current_hidden = hidden[-1]\n",
    "        mask = (src != self.hparams.pad_idx)\n",
    "        \n",
    "        for i in range(targets.size(1)):\n",
    "            attn_weights = self.attention(current_hidden, encoder_outputs, mask)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            combined = torch.cat((context, current_hidden), dim=1)\n",
    "            logits = self.fc(combined)\n",
    "            loss += self.loss_fn(logits, targets[:, i])\n",
    "            \n",
    "            inp_next = self.embedding(targets[:, i]).unsqueeze(1)\n",
    "            _, hidden = self.gru(inp_next, hidden)\n",
    "            current_hidden = hidden[-1]\n",
    "            \n",
    "        self.log('val_loss', loss / targets.size(1), prog_bar=True)\n",
    "\n",
    "# --- 4. LOAD & TRAIN ---\n",
    "# Load weights from your best previous checkpoint\n",
    "# Replace 'checkpoints/gru-attn-epoch=03...' with your actual best checkpoint path\n",
    "prev_ckpt = \"./checkpoints/daily-epoch=07-val_loss=4.44.ckpt\"\n",
    "\n",
    "print(\"Initializing Fine-Tuner...\")\n",
    "finetuner = MultiStepFineTuner.load_from_checkpoint(\n",
    "    prev_ckpt,\n",
    "    embedding_matrix=embedding_tensor,\n",
    "    strict=False # Allow strict=False in case of minor internal attribute diffs\n",
    ")\n",
    "\n",
    "# Create a new trainer for fine-tuning\n",
    "ft_trainer = pl.Trainer(\n",
    "    max_epochs=3, # Fine-tune for just a few epochs\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    precision='16-mixed',\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(dirpath='./checkpoints_ft', filename='finetuned-{epoch}-{val_loss_seq:.2f}'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Starting Fine-Tuning (Sequence Length 4)...\")\n",
    "ft_trainer.fit(finetuner, ft_train_loader, ft_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0867683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SANITY CHECK: Model Predictions (Skipping UNK)\n",
      "============================================================\n",
      "\n",
      "Input: 'how are you'\n",
      "Top 5 Predicted Next Words:\n",
      "  1. doing                (0.1802)\n",
      "  2. <EOS>                (0.0459)\n",
      "  3. feeling              (0.0385)\n",
      "  4. communicating        (0.0271)\n",
      "  5. going                (0.0251)\n",
      "\n",
      "Input: 'Nice meeting you'\n",
      "Top 5 Predicted Next Words:\n",
      "  1. cledus               (0.1341)\n",
      "  2. <EOS>                (0.0669)\n",
      "  3. tearing              (0.0636)\n",
      "  4. taransky             (0.0158)\n",
      "  5. and                  (0.0144)\n",
      "\n",
      "Input: 'this is a really'\n",
      "Top 5 Predicted Next Words:\n",
      "  1. staircase            (0.0349)\n",
      "  2. <EOS>                (0.0229)\n",
      "  3. barbaric             (0.0223)\n",
      "  4. mistake              (0.0204)\n",
      "  5. thing                (0.0204)\n",
      "\n",
      "Input: 'What is '\n",
      "Top 5 Predicted Next Words:\n",
      "  1. the                  (0.0797)\n",
      "  2. that                 (0.0593)\n",
      "  3. a                    (0.0447)\n",
      "  4. you                  (0.0375)\n",
      "  5. this                 (0.0341)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- SANITY CHECK ---\n",
    "def predict_next_words(model, partial_sentence, word2id, id2word, top_k=5, max_len=20):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    words = partial_sentence.split()\n",
    "    encoded = []\n",
    "    unk_id = int(word2id.get('<UNK>', 0))\n",
    "    \n",
    "    for w in words:\n",
    "        w = preprocess_word(w)\n",
    "        if not w: continue\n",
    "        encoded.append(int(word2id.get(w, unk_id)))\n",
    "    \n",
    "    if len(encoded) > max_len:\n",
    "        encoded = encoded[-max_len:]\n",
    "    \n",
    "    if not encoded:\n",
    "        return []\n",
    "\n",
    "    input_tensor = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    \n",
    "    probs = F.softmax(logits[0], dim=0)\n",
    "    \n",
    "    # Optional: Manually zero out UNK probability if you want strictly no UNK predictions\n",
    "    probs[unk_id] = 0.0\n",
    "    \n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n",
    "        word = id2word.get(str(int(idx)), '<UNK>')\n",
    "        results.append((word, float(prob)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_sentences = [\n",
    "    'how are you',\n",
    "    'Nice meeting you',\n",
    "    'this is a really',\n",
    "    'What is '\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SANITY CHECK: Model Predictions (Skipping UNK)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predictions = predict_next_words(model, sentence, word2id, id2word, top_k=5)\n",
    "    print(f\"\\nInput: '{sentence}'\")\n",
    "    print(\"Top 5 Predicted Next Words:\")\n",
    "    for i, (word, prob) in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {word:<20} ({prob:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b674fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoints/daily-epoch=04-val_loss=4.96.ckpt...\n",
      "Model loaded successfully!\n",
      "\n",
      "========================================\n",
      "ðŸ¤– TEXT COMPLETION BOT (Type 'exit' to stop)\n",
      "========================================\n",
      "Model: who \u001b[1m\u001b[0m\n",
      "Model: who are \u001b[1myou\u001b[0m\n",
      "Model: what do  \u001b[1myou think\u001b[0m\n",
      "Model: i love \u001b[1mthe guy\u001b[0m\n",
      "Model: kiss \u001b[1m\u001b[0m\n",
      "Model: kiss me \u001b[1m\u001b[0m\n",
      "Model: can you \u001b[1mbe a fool\u001b[0m\n",
      "Model: yo \u001b[1m\u001b[0m\n",
      "Model: you \u001b[1mdon't know what you think\u001b[0m\n",
      "Model: I hate \u001b[1mthe truth\u001b[0m\n",
      "Model: can we \u001b[1mtalk\u001b[0m\n",
      "Model: what abouy \u001b[1m\u001b[0m\n",
      "Model: what about \u001b[1mthe other\u001b[0m\n",
      "Model: I don't \u001b[1mknow what i mean\u001b[0m\n",
      "Model: he is  \u001b[1ma man\u001b[0m\n",
      "Model: she is  \u001b[1ma very very very fifty-six of a young\u001b[0m\n",
      "Model: are you \u001b[1mkidding\u001b[0m\n",
      "Model: we need \u001b[1mto get a lot\u001b[0m\n",
      "Model: huh \u001b[1m\u001b[0m\n",
      "Model: wow \u001b[1m\u001b[0m\n",
      "Model: I hate the \u001b[1mtruth\u001b[0m\n",
      "Model: i quit \u001b[1myou\u001b[0m\n",
      "Model: what is  \u001b[1mthe name\u001b[0m\n",
      "Model: what is the name \u001b[1m\u001b[0m\n",
      "Model: what is the name of \u001b[1mthe name\u001b[0m\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- Load Checkpoint and Interactive Generation ---\n",
    "\n",
    "# 1. Path to your checkpoint\n",
    "# Make sure the path matches where the file is actually located.\n",
    "# Based on the previous cell, it's likely inside the 'checkpoints' folder.\n",
    "ckpt_path = \"./checkpoints/daily-epoch=04-val_loss=4.96.ckpt\"\n",
    "\n",
    "print(f\"Loading model from {ckpt_path}...\")\n",
    "\n",
    "# 2. Load the Model\n",
    "# We must pass 'embedding_matrix' because we ignored it in save_hyperparameters\n",
    "loaded_model = NextWordGRU.load_from_checkpoint(\n",
    "    ckpt_path,\n",
    "    embedding_matrix=embedding_tensor,  # Requires embedding_tensor from previous cells\n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "\n",
    "# 3. Define Autoregressive Generation Function\n",
    "def generate_completion(\n",
    "    model, start_text, word2id, id2word, max_generated=20, temp=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates text starting from start_text until <EOS> or max_generated tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words = start_text.split()\n",
    "    current_ids = []\n",
    "\n",
    "    # Encode initial string\n",
    "    unk_id = int(word2id.get(\"<UNK>\", 0))\n",
    "    for w in words:\n",
    "        w = preprocess_word(w)\n",
    "        if w:\n",
    "            current_ids.append(int(word2id.get(w, unk_id)))\n",
    "\n",
    "    input_seq = current_ids[:]  # Copy for keeping track\n",
    "    generated_words = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_generated):\n",
    "            # Prepare input tensor (truncate to max_seq_len if needed)\n",
    "            seq_tensor = torch.tensor([input_seq[-20:]], dtype=torch.long).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(seq_tensor)\n",
    "\n",
    "            # Apply Temperature (Higher = more random/creative, Lower = more deterministic)\n",
    "            logits = logits[0] / temp\n",
    "\n",
    "            # Get probabilities\n",
    "            probs = F.softmax(logits, dim=0)\n",
    "\n",
    "            # Sample from the distribution (more natural) or take Argmax (more rigid)\n",
    "            # Using Argmax for stability in early training, change to multinomial for creativity\n",
    "            next_token_id = torch.argmax(probs).item()\n",
    "\n",
    "            # Decode\n",
    "            next_word = id2word.get(str(next_token_id), \"<UNK>\")\n",
    "\n",
    "            # Stop if EOS or Unknown (optional)\n",
    "            if next_word == \"<EOS>\":\n",
    "                break\n",
    "\n",
    "            generated_words.append(next_word)\n",
    "            input_seq.append(next_token_id)\n",
    "\n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "\n",
    "# 4. Interactive Loop\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"ðŸ¤– TEXT COMPLETION BOT (Type 'exit' to stop)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter start of sentence: \")\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    if not user_input.strip():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        completion = generate_completion(\n",
    "            loaded_model, user_input, word2id, id2word, max_generated=8\n",
    "        )\n",
    "        print(f\"Model: {user_input} \\033[1m{completion}\\033[0m\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
