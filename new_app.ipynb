{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011adbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab and embeddings...\n",
      "Vocab Size: 10003\n",
      "Embedding Shape: torch.Size([10003, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Loading Resources\n",
    "# ==========================================\n",
    "\n",
    "class Config:\n",
    "    # Files provided by user\n",
    "    EMBED_PATH = 'embedding_matrix.npz'\n",
    "    W2I_PATH = 'word2id.json'\n",
    "    I2W_PATH = 'id2word.json'\n",
    "    DATA_PATH = 'processed_reddit.txt'\n",
    "    \n",
    "    # Model Hyperparameters\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_LAYERS = 2     # Stacked GRU\n",
    "    DROPOUT = 0.3\n",
    "    BATCH_SIZE = 256\n",
    "    LEARNING_RATE = 1e-3\n",
    "    MAX_LEN = 30       # Max sequence length for sanity (optional clipping)\n",
    "\n",
    "print(\"Loading vocab and embeddings...\")\n",
    "\n",
    "# 1. Load Mappings\n",
    "with open(Config.W2I_PATH, 'r') as f:\n",
    "    raw_w2i = json.load(f)\n",
    "    # CHANGE IS HERE: Explicitly convert values to int\n",
    "    # This handles cases where json is {\"word\": \"5\"} or {\"word\": 5}\n",
    "    word2id = {k: int(v) for k, v in raw_w2i.items()}\n",
    "\n",
    "with open(Config.I2W_PATH, 'r') as f:\n",
    "    raw_i2w = json.load(f)\n",
    "    # Standard JSON keys are always strings, so we convert keys to int\n",
    "    id2word = {int(k): v for k, v in raw_i2w.items()}\n",
    "\n",
    "# 2. Load Embeddings\n",
    "# Assuming the file has a key like 'embeddings' or 'arr_0'\n",
    "loaded_embeds = np.load(Config.EMBED_PATH)\n",
    "# If npz contains multiple arrays, usually 'arr_0' is the default if not named\n",
    "embedding_matrix = loaded_embeds['arr_0'] if 'arr_0' in loaded_embeds else loaded_embeds[list(loaded_embeds.keys())[0]]\n",
    "\n",
    "# Convert to torch tensor\n",
    "embedding_tensor = torch.from_numpy(embedding_matrix).float()\n",
    "\n",
    "# Identify special tokens\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "\n",
    "PAD_ID = word2id.get(PAD_TOKEN, 0)\n",
    "UNK_ID = word2id.get(UNK_TOKEN, 1)\n",
    "EOS_ID = word2id.get(EOS_TOKEN, 2)\n",
    "\n",
    "VOCAB_SIZE = len(word2id)\n",
    "\n",
    "print(f\"Vocab Size: {VOCAB_SIZE}\")\n",
    "print(f\"Embedding Shape: {embedding_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9132e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Dataset & Collate Function\n",
    "# ==========================================\n",
    "\n",
    "class DailyDialogDataset(Dataset):\n",
    "    def __init__(self, file_path, w2i, max_len=50):\n",
    "        self.sentences = []\n",
    "        self.w2i = w2i\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    self.sentences.append(line)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.sentences[idx]\n",
    "        \n",
    "        # Simple tokenization by space (adjust if your corpus needs complex tokenization)\n",
    "        words = text.split()\n",
    "        \n",
    "        # Convert to IDs, using UNK_ID if word not found\n",
    "        token_ids = [self.w2i.get(w, UNK_ID) for w in words]\n",
    "        \n",
    "        # Append EOS\n",
    "        token_ids.append(EOS_ID)\n",
    "        \n",
    "        # Optional: Clip to max length\n",
    "        if len(token_ids) > self.max_len:\n",
    "            token_ids = token_ids[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "def completion_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable lengths and padding.\n",
    "    Prepares Input (X) and Target (Y).\n",
    "    \"\"\"\n",
    "    # Pad sequences to the longest in the batch\n",
    "    # batch is a list of tensors [seq_len]\n",
    "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=PAD_ID)\n",
    "    \n",
    "    # Create Inputs and Targets for Next-Word Prediction\n",
    "    # Input:  [w1, w2, w3]\n",
    "    # Target: [w2, w3, EOS]\n",
    "    \n",
    "    inputs = padded_batch[:, :-1]\n",
    "    targets = padded_batch[:, 1:]\n",
    "    \n",
    "    # Create a clone for loss calculation\n",
    "    loss_targets = targets.clone()\n",
    "    \n",
    "    # LOGIC: If target is <UNK>, skip it (set to ignore_index -100)\n",
    "    # LOGIC: If target is <PAD>, skip it (set to ignore_index -100)\n",
    "    loss_targets[loss_targets == UNK_ID] = -100\n",
    "    loss_targets[loss_targets == PAD_ID] = -100\n",
    "    \n",
    "    return inputs, loss_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd83d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Model Architecture (Stacked GRU + Attention)\n",
    "# ==========================================\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple dot-product self-attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs: [Batch, Seq_Len, Hidden_Dim]\n",
    "        \n",
    "        # Calculate energy\n",
    "        # energy: [Batch, Seq_Len, 1]\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        \n",
    "        # Calculate weights\n",
    "        weights = F.softmax(energy.squeeze(-1), dim=1) # [Batch, Seq_Len]\n",
    "        \n",
    "        # Apply weights to outputs to get context vector\n",
    "        # weights.unsqueeze(1): [Batch, 1, Seq_Len]\n",
    "        # encoder_outputs: [Batch, Seq_Len, Hidden_Dim]\n",
    "        # bmm result: [Batch, 1, Hidden_Dim]\n",
    "        context = torch.bmm(weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        return context.squeeze(1), weights\n",
    "\n",
    "class CompletionModel(pl.LightningModule):\n",
    "    def __init__(self, embedding_tensor, hidden_dim, num_layers, dropout, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['embedding_tensor'])\n",
    "        \n",
    "        self.vocab_size = embedding_tensor.size(0)\n",
    "        self.embed_dim = embedding_tensor.size(1)\n",
    "        \n",
    "        # 1. Embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_tensor, \n",
    "            freeze=False, \n",
    "            padding_idx=PAD_ID\n",
    "        )\n",
    "        \n",
    "        # 2. Stacked GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 3. Attention & Output\n",
    "        self.attention = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, self.vocab_size)\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        # Loss function (ignores -100)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        attn_out = torch.tanh(self.attention(gru_out))\n",
    "        out = self.layer_norm(gru_out + attn_out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        logits = self(inputs) \n",
    "        \n",
    "        # Flatten\n",
    "        loss = self.criterion(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "        \n",
    "        # Logs training loss per step\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        logits = self(inputs) # [Batch, Seq, Vocab]\n",
    "        \n",
    "        # 1. Calculate Validation Loss\n",
    "        loss = self.criterion(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "        \n",
    "        # 2. Calculate Validation Accuracy\n",
    "        # Get the predicted token ID\n",
    "        preds = torch.argmax(logits, dim=-1) # [Batch, Seq]\n",
    "        \n",
    "        # Create a mask of valid targets (tokens that are NOT -100)\n",
    "        # We did targets[targets == UNK_ID] = -100 in the collate_fn\n",
    "        mask = (targets != -100) \n",
    "        \n",
    "        # Count how many predictions matched the targets, strictly where mask is True\n",
    "        correct_predictions = ((preds == targets) & mask).sum()\n",
    "        \n",
    "        # Total number of valid tokens (excluding PAD and UNK)\n",
    "        total_valid_tokens = mask.sum()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if total_valid_tokens > 0:\n",
    "            accuracy = correct_predictions.float() / total_valid_tokens.float()\n",
    "        else:\n",
    "            # Ensure the 0.0 is on the same device as the model/targets\n",
    "            accuracy = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        # Log metrics (on_epoch=True ensures it averages over the whole val set)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', accuracy, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f6204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2886695\n",
      "Validation samples: 320744\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. Training\n",
    "# ==========================================\n",
    "\n",
    "# Prepare Data\n",
    "full_dataset = DailyDialogDataset(Config.DATA_PATH, word2id)\n",
    "\n",
    "# Split 90% Train, 10% Val\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Train Loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=completion_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Validation Loader (Shuffle=False usually)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=completion_collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize Model\n",
    "model = CompletionModel(\n",
    "    embedding_tensor=embedding_tensor,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    num_layers=Config.NUM_LAYERS,\n",
    "    dropout=Config.DROPOUT,\n",
    "    lr=Config.LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    # Optional: Stop early if validation loss doesn't improve\n",
    "    callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5081a1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"lightning_logs/version_6/checkpoints/epoch=9-step=112770.ckpt\")\n",
    "model = CompletionModel(\n",
    "    embedding_tensor=embedding_tensor,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    num_layers=Config.NUM_LAYERS,\n",
    "    dropout=Config.DROPOUT,\n",
    "    lr=Config.LEARNING_RATE\n",
    ")\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training with Validation\n",
    "print(\"Starting Training...\")\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a52a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Robust Sanity Check...\n",
      "how are you supposed to be in a relationship\n",
      "\n",
      "=== Sanity Check: Auto-completion ===\n",
      "Input: 'how are you'  ->  Output: 'how are you supposed to be in a relationship'\n",
      "Input: 'what is your name'  ->  Output: 'what is your name '\n",
      "Input: 'i would like to buy'  ->  Output: 'i would like to buy a house and have a good time'\n",
      "Input: 'the weather is very'  ->  Output: 'the weather is very cold and i have a lot of fun'\n",
      "Input: 'can you help me'  ->  Output: 'can you help me '\n",
      "Input: 'my name '  ->  Output: 'my name  is a little bit of a question'\n",
      "Input: 'this might'  ->  Output: 'this might be a bit of a rant but i don't know'\n"
     ]
    }
   ],
   "source": [
    "def complete_sentence_robust(model, start_text, max_new_words=10):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Detect the device of the model parameters (CPU or CUDA)\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    words = start_text.split()\n",
    "    # Use UNK_ID if word is missing\n",
    "    current_ids = [word2id.get(w, word2id.get('<UNK>')) for w in words]\n",
    "    \n",
    "    # 2. Create tensor and EXPLICITLY move it to the detected device\n",
    "    current_tensor = torch.tensor([current_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_words = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_words):\n",
    "            logits = model(current_tensor)\n",
    "            \n",
    "            # Get logits of the last token\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Greedy prediction\n",
    "            predicted_id = torch.argmax(last_token_logits).item()\n",
    "            \n",
    "            # Stop at EOS\n",
    "            if predicted_id == word2id.get('<EOS>'):\n",
    "                break\n",
    "                \n",
    "            predicted_word = id2word.get(predicted_id, '<UNK>')\n",
    "            generated_words.append(predicted_word)\n",
    "            \n",
    "            # 3. Create next input and move to SAME device\n",
    "            next_input = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "            current_tensor = torch.cat([current_tensor, next_input], dim=1)\n",
    "            \n",
    "    return start_text + \" \" + \" \".join(generated_words)\n",
    "\n",
    "# Run the sanity check again\n",
    "print(\"Running Robust Sanity Check...\")\n",
    "print(complete_sentence_robust(model, \"how are you\"))\n",
    "\n",
    "print(\"\\n=== Sanity Check: Auto-completion ===\")\n",
    "test_sentences = [\n",
    "    \"how are you\",\n",
    "    \"what is your name\",\n",
    "    \"i would like to buy\",\n",
    "    \"the weather is very\",\n",
    "    \"can you help me\",\n",
    "    \"my name \",\n",
    "    \"this might\"\n",
    "]\n",
    "\n",
    "# Move model to CPU for quick inference check if it was on GPU\n",
    "model.to('cpu')\n",
    "\n",
    "for sent in test_sentences:\n",
    "    completion = complete_sentence_robust(model, sent)\n",
    "    print(f\"Input: '{sent}'  ->  Output: '{completion}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d0f9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Interactive Auto-Completion Mode\n",
      "Type a start of a sentence (e.g., 'how are') and hit Enter.\n",
      "Type 'quit', 'exit', or 'q' to stop.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: i am -> Model: i am a year old male and i am a year old male\n",
      "user: i am an -> Model: i am an atheist and i am a very religious person\n",
      "user: can we -> Model: can we be able to get back together\n",
      "user: are you in -> Model: are you in the wrong\n",
      "user: people are -> Model: people are not the same\n",
      "user: i love -> Model: i love her and i don't want to lose her\n",
      "user: i want to buy -> Model: i want to buy a house and have a good time\n",
      "user: this is nice -> Model: this is nice \n",
      "user: can we talk -> Model: can we talk about it\n",
      "user: i hate -> Model: i hate it\n",
      "user: honestly -> Model: honestly i don't know what to do\n",
      "user: finally -> Model: finally i was able to get my head back and i was in the middle of\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# INTERACTIVE DEMO CELL\n",
    "# ==========================================\n",
    "\n",
    "import torch\n",
    "\n",
    "def interactive_completion(model, w2i, i2w, device, max_new_words=15):\n",
    "    \"\"\"\n",
    "    Runs an interactive loop for auto-completion.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Interactive Auto-Completion Mode\")\n",
    "    print(\"Type a start of a sentence (e.g., 'how are') and hit Enter.\")\n",
    "    print(\"Type 'quit', 'exit', or 'q' to stop.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_text = input(\"\\nInput: \").strip().lower()\n",
    "            \n",
    "            # Check for exit command\n",
    "            if user_text in ['quit', 'exit', 'q']:\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "            \n",
    "            if not user_text:\n",
    "                continue\n",
    "\n",
    "            # Tokenize and convert to IDs\n",
    "            words = user_text.split()\n",
    "            current_ids = [w2i.get(w, w2i.get('<UNK>')) for w in words]\n",
    "            \n",
    "            # Convert to tensor [1, seq_len]\n",
    "            current_tensor = torch.tensor([current_ids], dtype=torch.long).to(device)\n",
    "            \n",
    "            generated_words = []\n",
    "            \n",
    "            # Generation Loop\n",
    "            with torch.no_grad():\n",
    "                for _ in range(max_new_words):\n",
    "                    # Forward pass\n",
    "                    logits = model(current_tensor)\n",
    "                    \n",
    "                    # Get the logits for the last step: [Batch, Seq, Vocab] -> [Vocab]\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    \n",
    "                    # Greedy decoding (Pick best prob)\n",
    "                    # For more variety, use: predicted_id = torch.multinomial(F.softmax(next_token_logits, dim=-1), 1).item()\n",
    "                    predicted_id = torch.argmax(next_token_logits).item()\n",
    "                    \n",
    "                    # Stop if EOS\n",
    "                    if predicted_id == w2i.get('<EOS>'):\n",
    "                        break\n",
    "                    \n",
    "                    # Decode word\n",
    "                    predicted_word = i2w.get(predicted_id, '<UNK>')\n",
    "                    generated_words.append(predicted_word)\n",
    "                    \n",
    "                    # Prepare input for next step (feed back the predicted token)\n",
    "                    next_input = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "                    current_tensor = torch.cat([current_tensor, next_input], dim=1)\n",
    "            \n",
    "            # Print result\n",
    "            full_sentence = user_text + \" \" + \" \".join(generated_words)\n",
    "            print(f\"user: {user_text} -> Model: {full_sentence}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrupted by user.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Run the interactive session\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "interactive_completion(model, word2id, id2word, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43373ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully exported to completion_model_reddit.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatem/.virtualenvs/ml/lib/python3.13/site-packages/torch/onnx/symbolic_opset9.py:4277: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# EXPORT TO ONNX\n",
    "# ==========================================\n",
    "import torch\n",
    "\n",
    "# 1. Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# 2. Create a dummy input (Batch size 1, Sequence length 5)\n",
    "# The values don't matter, just the shape and type (Long/Int64)\n",
    "dummy_input = torch.randint(0, 100, (1, 5), dtype=torch.long).to(device)\n",
    "\n",
    "# 3. Define path\n",
    "onnx_file_path = \"completion_model_reddit.onnx\"\n",
    "\n",
    "# 4. Export\n",
    "# dynamic_axes is CRITICAL: it tells ONNX that the sequence length (dim 1) \n",
    "# can change, so you can feed it 2 words or 20 words.\n",
    "torch.onnx.export(\n",
    "    model,                      # Model instance\n",
    "    dummy_input,                # Dummy input\n",
    "    onnx_file_path,             # Output file\n",
    "    export_params=True,         # Store the trained parameter weights\n",
    "    opset_version=12,           # Standard opset (11 or 12 works best for GRU/Attention)\n",
    "    do_constant_folding=True,   # Optimization\n",
    "    input_names=['input_ids'],  # Name of input layer\n",
    "    output_names=['logits'],    # Name of output layer\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'logits':    {0: 'batch_size', 1: 'sequence_length'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model successfully exported to {onnx_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
