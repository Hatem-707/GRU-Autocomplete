{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21571749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'vocab_path': 'word2id_daily.json',\n",
    "    'id_path': 'id2word_daily.json',\n",
    "    'emb_path': 'embedding_matrix_daily.npz',\n",
    "    'batch_size': 512,\n",
    "    'hidden_dim': 512,       # Reverted to 256 as requested\n",
    "    'num_layers': 3,         # IMPROVEMENT: Increased layers for depth\n",
    "    'learning_rate': 0.001,\n",
    "    'max_seq_len': 30, \n",
    "    'pad_token_id': 0,\n",
    "    'dropout': 0.3,\n",
    "    'unk_token_id': 0        # Placeholder, will update after loading vocab\n",
    "}\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a556a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch, hidden_dim] (The final state of the GRU)\n",
    "        # encoder_outputs: [batch, seq_len, hidden_dim] (All states of the GRU)\n",
    "        \n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden state src_len times\n",
    "        hidden_expanded = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(hidden_expanded + encoder_outputs))\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e4)\n",
    "            \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017f220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordGRU(pl.LightningModule):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, vocab_size, lr, pad_idx):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['embedding_matrix'])\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        # FROZEN as requested because they are FastText semantics\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix, \n",
    "            freeze=True, \n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # 2. GRU Layer (Reverted to GRU)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_matrix.shape[1], \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=CONFIG['num_layers'], # 3 Layers\n",
    "            batch_first=True,\n",
    "            dropout=CONFIG['dropout'] if CONFIG['num_layers'] > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 3. Attention Layer\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # 4. Dense Output\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != self.hparams.pad_idx)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # GRU Output\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        \n",
    "        # Take the hidden state of the LAST layer\n",
    "        final_hidden = hidden[-1] \n",
    "        \n",
    "        # Calculate Attention\n",
    "        attn_weights = self.attention(final_hidden, outputs, mask)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), outputs).squeeze(1)\n",
    "        \n",
    "        # Combine Context and Hidden\n",
    "        combined = torch.cat((context, final_hidden), dim=1)\n",
    "        logits = self.fc(combined)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predictions == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7e4dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary and embedding matrix...\n",
      "Vocabulary size: 20003\n",
      "Embedding matrix shape: torch.Size([20003, 300])\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary and embedding matrix\n",
    "print(\"Loading vocabulary and embedding matrix...\")\n",
    "with open(CONFIG['vocab_path'], 'r') as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "with open(CONFIG['id_path'], 'r') as f:\n",
    "    id2word = json.load(f)\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Load embedding matrix\n",
    "embedding_data = np.load(CONFIG['emb_path'])\n",
    "embedding_matrix = torch.from_numpy(embedding_data['embedding_matrix']).float()\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# Update unknown token id in config\n",
    "CONFIG['unk_token_id'] = word2id.get('<unk>', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e54589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: checkpoints/daily-epoch=07-val_loss=4.41.ckpt\n",
      "Model loaded and set to eval mode\n",
      "Model loaded and set to eval mode\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = 'checkpoints/daily-epoch=07-val_loss=4.41.ckpt'\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "# Load the Lightning module from checkpoint\n",
    "model = NextWordGRU.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    hidden_dim=CONFIG['hidden_dim'],\n",
    "    vocab_size=vocab_size,\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    pad_idx=CONFIG['pad_token_id']\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "print(\"Model loaded and set to eval mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4448516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: model_daily_gru_attention.pth\n",
      "✓ Model saved as .pth: model_daily_gru_attention.pth\n",
      "  File size: 143.00 MB\n",
      "✓ Model saved as .pth: model_daily_gru_attention.pth\n",
      "  File size: 143.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Save as .pth (PyTorch state dict + metadata)\n",
    "pth_save_path = 'model_daily_gru_attention.pth'\n",
    "print(f\"Saving model to: {pth_save_path}\")\n",
    "\n",
    "# Save the model state dict along with embedding matrix and config\n",
    "checkpoint_data = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'embedding_matrix': embedding_matrix,\n",
    "    'vocab_size': vocab_size,\n",
    "    'vocab': word2id,\n",
    "    'id2word': id2word,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "torch.save(checkpoint_data, pth_save_path)\n",
    "print(f\"✓ Model saved as .pth: {pth_save_path}\")\n",
    "print(f\"  File size: {os.path.getsize(pth_save_path) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daccbc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ONNX-compatible model wrapper\n"
     ]
    }
   ],
   "source": [
    "# Create a wrapper for ONNX export (without the Lightning wrapper)\n",
    "class NextWordGRUForONNX(nn.Module):\n",
    "    \"\"\"Pure PyTorch model without Lightning - suitable for ONNX export\"\"\"\n",
    "    def __init__(self, model: NextWordGRU):\n",
    "        super().__init__()\n",
    "        self.embedding = model.embedding\n",
    "        self.gru = model.gru\n",
    "        self.attention = model.attention\n",
    "        self.fc = model.fc\n",
    "        self.pad_idx = model.hparams.pad_idx\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mask = (x != self.pad_idx)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # GRU Output\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        \n",
    "        # Take the hidden state of the LAST layer\n",
    "        final_hidden = hidden[-1]\n",
    "        \n",
    "        # Calculate Attention\n",
    "        attn_weights = self.attention(final_hidden, outputs, mask)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), outputs).squeeze(1)\n",
    "        \n",
    "        # Combine Context and Hidden\n",
    "        combined = torch.cat((context, final_hidden), dim=1)\n",
    "        logits = self.fc(combined)\n",
    "        return logits\n",
    "\n",
    "# Create the pure PyTorch model\n",
    "onnx_model = NextWordGRUForONNX(model)\n",
    "onnx_model.eval()\n",
    "onnx_model = onnx_model.to(device)\n",
    "\n",
    "print(\"Created ONNX-compatible model wrapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "977a2442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX: model_daily_gru_attention.onnx\n",
      "✓ Model exported to ONNX: model_daily_gru_attention.onnx\n",
      "  File size: 118.91 MB\n",
      "✓ Model exported to ONNX: model_daily_gru_attention.onnx\n",
      "  File size: 118.91 MB\n"
     ]
    }
   ],
   "source": [
    "# Export to ONNX format for edge deployment\n",
    "onnx_save_path = 'model_daily_gru_attention.onnx'\n",
    "print(f\"Exporting model to ONNX: {onnx_save_path}\")\n",
    "\n",
    "# Create a dummy input for tracing\n",
    "dummy_input = torch.randint(0, vocab_size, (1, CONFIG['max_seq_len'])).to(device)\n",
    "\n",
    "try:\n",
    "    # Export the model\n",
    "    torch.onnx.export(\n",
    "        onnx_model,\n",
    "        dummy_input,\n",
    "        onnx_save_path,\n",
    "        input_names=['input_ids'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={\n",
    "            'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "            'logits': {0: 'batch_size'}\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(f\"✓ Model exported to ONNX: {onnx_save_path}\")\n",
    "    print(f\"  File size: {os.path.getsize(onnx_save_path) / (1024**2):.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during ONNX export: {e}\")\n",
    "    print(\"  Note: Some layers might not be fully supported in ONNX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab1b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verification ===\n",
      "✓ .pth file exists: True\n",
      "✓ ONNX file exists: True\n",
      "\n",
      "Testing inference...\n",
      "✓ Model output shape: torch.Size([2, 20003])\n",
      "  Expected: torch.Size([2, 20003])\n",
      "\n",
      "=== Export Summary ===\n",
      "1. PyTorch model (.pth): model_daily_gru_attention.pth\n",
      "   - Contains: model weights + embedding matrix + vocabulary + config\n",
      "2. ONNX model: model_daily_gru_attention.onnx\n",
      "   - Format: ONNX opset 14 (compatible with TensorRT, ONNX Runtime)\n",
      "\n",
      "Both models are ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Verification: Test the saved .pth model\n",
    "print(\"\\n=== Verification ===\")\n",
    "print(f\"✓ .pth file exists: {os.path.exists(pth_save_path)}\")\n",
    "print(f\"✓ ONNX file exists: {os.path.exists(onnx_save_path)}\")\n",
    "\n",
    "# Test inference with dummy input\n",
    "print(\"\\nTesting inference...\")\n",
    "test_input = torch.randint(0, vocab_size, (2, CONFIG['max_seq_len'])).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    print(f\"✓ Model output shape: {output.shape}\")\n",
    "    print(f\"  Expected: torch.Size([2, {vocab_size}])\")\n",
    "\n",
    "print(\"\\n=== Export Summary ===\")\n",
    "print(f\"1. PyTorch model (.pth): {pth_save_path}\")\n",
    "print(f\"   - Contains: model weights + embedding matrix + vocabulary + config\")\n",
    "print(f\"2. ONNX model: {onnx_save_path}\")\n",
    "print(f\"   - Format: ONNX opset 14 (compatible with TensorRT, ONNX Runtime)\")\n",
    "print(f\"\\nBoth models are ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
